---
layout: page
title: Analysis and Experiments
subtitle: Useful framing for escaping the noise.
---

---

# Data Analysis
Framing, Cleaning, Approaches

## Framing

[**Tukey, Design Thinking, and Better Questions**](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/)

> In my view, the most useful thing a data scientist can do is to devote serious effort towards improving the quality and sharpness of the question being asked.

[**Trustworthy Data Analysis**](https://simplystatistics.org/2018/06/04/trustworthy-data-analysis/)

> It’s entirely possible to trust an analysis but not believe the final conclusions.

[**20 Questions to Ask Prior to Starting Data Analysis**](https://towardsdatascience.com/20-questions-to-ask-prior-to-starting-data-analysis-6ec11d6a504b)

[**Research Design Patterns**](http://pgbovine.net/research-design-patterns.htm)

## Cleaning / Tidying / Munging / Wrangling

[**Tidy Data**](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)

> The principles of tidy data provide a standard way to organize data values within a dataset.

[**The Quartz guide to bad data**](https://github.com/Quartz/bad-data-guide)

> An exhaustive reference to problems seen in real-world data along with suggestions on how to resolve them.

## Approaches

[**Importance of Skepticism in Data Science**](https://jhu-advdatasci.github.io/2018/lectures/12-being-skeptical.html)

> Here we examine three common ways that can lead to misinterpreting data.

[**Conversion rates – you are (most likely) computing them wrong**](https://erikbern.com/2017/05/23/conversion-rates-you-are-most-likely-computing-them-wrong.html)

[**The Power User Curve: The best way to understand your most engaged users**](https://andrewchen.co/power-user-curve/)

[**Forecasting at scale**](https://peerj.com/preprints/3190/)

[**Using Causal Inference to Improve the Uber User Experience**](https://eng.uber.com/causal-inference-at-uber/)

[**Inferring causal impact using Bayesian structural time-series models**](https://ai.google/research/pubs/pub41854)

[**The 10 Statistical Techniques Data Scientists Need to Master**](https://medium.com/cracking-the-data-science-interview/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7)

[**Fundamentals of Data Visualization (book)**](https://serialmentor.com/dataviz/index.html)

[**Forecasting: Principles and Practice (book)**](https://otexts.com/fpp2/)

---

# Experimentation

Framing, Approaches, Significance Pitfalls

## Framing

[**Common statistical tests are linear models (or: how to teach stats)**](https://lindeloev.github.io/tests-as-linear/)

[**North Star or sign post metrics: which should one optimize?**](https://medium.com/@leapingllamas/north-star-or-sign-post-metrics-which-should-one-optimize-24bcc9c05bfb)

[**How Not To Run an A/B Test**](http://www.evanmiller.org/how-not-to-run-an-ab-test.html)

[**Misadventures in experiments for growth**](http://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html)

[**The Agony and Ecstasy of Building with Data**](https://medium.com/the-year-of-the-looking-glass/the-agony-and-ecstasy-of-building-with-data-56215764d67c)

[**Against A/B Tests**](https://www.locallyoptimistic.com/post/against-ab-tests/)

## Approaches

[**Guidelines for A/B Testing**](https://hookedondata.org/guidelines-for-ab-testing/)

[**How Etsy Handles Peeking in A/B Testing**](https://codeascraft.com/2018/10/03/how-etsy-handles-peeking-in-a-b-testing/)

[**Suffering from a Non-inferiority Complex?**](https://multithreaded.stitchfix.com/blog/2019/05/06/noninferiority/)

[**Analyzing Experiment Outcomes: Beyond Average Treatment Effects**](https://eng.uber.com/analyzing-experiment-outcomes/)

[**Mediation Modeling at Uber: Understanding Why Product Changes Work (and Don’t Work)**](https://eng.uber.com/mediation-modeling/)

> Mediation modeling goes beyond simple cause and effect relationships in an attempt to understand what underlying mechanisms led to a result.

[**Experimentation & Measurement for Search Engine Optimization**](https://medium.com/airbnb-engineering/experimentation-measurement-for-search-engine-optimization-b64136629760)

## Statistical Significance Pitfalls

[**The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time**](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf)

> Our key point here is that it is possible to have multiple potential comparisons, in the sense of a data analysis whose details are highly contingent on data, without the researcher performing any conscious procedure of fishing or examining multiple p-values.

[**False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant**](https://journals.sagepub.com/doi/full/10.1177/0956797611417632?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub%3Dpubmed)

[**Why Most Published Research Findings Are False**](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)

[**The Control Group is Out of Control**](https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/)

[**Beware The Man of One Study**](https://slatestarcodex.com/2014/12/12/beware-the-man-of-one-study/)

> Decrease your confidence about most things if you’re not sure that you’ve investigated every piece of evidence.
